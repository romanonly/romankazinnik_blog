apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: train-amazon-pipeline-
  annotations:
    pipelines.kubeflow.org/kfp_sdk_version: 1.7.0
    pipelines.kubeflow.org/pipeline_compilation_time: '2022-12-30T08:24:37.286572'
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"default": "10", "name": "epochs",
      "optional": true, "type": "Integer"}, {"default": "12", "name": "batch_size",
      "optional": true, "type": "Integer"}, {"default": "10000", "name": "num_samples",
      "optional": true, "type": "Integer"}, {"default": "https://www.dropbox.com/s/tdsek2g4jwfoy8q/train.csv?dl=1",
      "name": "url_train", "optional": true, "type": "String"}, {"default": "https://www.dropbox.com/s/tdsek2g4jwfoy8q/test.csv?dl=1",
      "name": "url_test", "optional": true, "type": "String"}, {"default": "", "name":
      "pipeline-output-directory"}, {"default": "pipeline/train_amazon_pipeline",
      "name": "pipeline-name"}], "name": "train_amazon_pipeline"}'
    pipelines.kubeflow.org/v2_pipeline: "true"
  labels:
    pipelines.kubeflow.org/v2_pipeline: "true"
    pipelines.kubeflow.org/kfp_sdk_version: 1.7.0
spec:
  entrypoint: train-amazon-pipeline
  templates:
  - name: eval-model
    container:
      args:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location
        'tensorflow' 'pandas' 'kfp==1.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet                 --no-warn-script-location 'tensorflow'
        'pandas' 'kfp==1.7.0' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        from kfp.v2.dsl import *
        from typing import *

        def eval_model(input_model: Input[Artifact], input_labels_artifacts: Input[Artifact],
                       input_text_artifacts: Input[Artifact], output_metrics: Output[Dataset]):
            import tensorflow as tf
            import pickle
            import pandas as pd

            with open(input_labels_artifacts.path, "rb") as file:
                y_eval = pickle.load(file)

            with open(input_text_artifacts.path, "rb") as file:
                x_eval = pickle.load(file)

            model = tf.keras.models.load_model(input_model.path)

            # Test the model and print loss and mse for both outputs
            loss, acc = model.evaluate(x = x_eval, y = y_eval)
            print("\n\n\n Loss = {}, Acc = {} ".format(loss, acc))
            metrics = pd.DataFrame(
                list(
                    zip(
                        ['loss', 'accuracy'], [loss, acc]
                    )
                ), columns = ['Name', 'val']
            )
            metrics.to_csv(output_metrics.path, index = False)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - eval_model
      - --input-model-output-path
      - '{{$.inputs.artifacts[''input_model''].path}}'
      - --input-labels-artifacts-output-path
      - '{{$.inputs.artifacts[''input_labels_artifacts''].path}}'
      - --input-text-artifacts-output-path
      - '{{$.inputs.artifacts[''input_text_artifacts''].path}}'
      - --output-metrics-output-path
      - '{{$.outputs.artifacts[''output_metrics''].path}}'
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, eval-model, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-output-directory}}',
        --enable_caching, $(ENABLE_CACHING), --, --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.7'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {}, "inputArtifacts":
          {"input_labels_artifacts": {"metadataPath": "/tmp/inputs/input_labels_artifacts/data",
          "schemaTitle": "system.Artifact", "instanceSchema": ""}, "input_model":
          {"metadataPath": "/tmp/inputs/input_model/data", "schemaTitle": "system.Artifact",
          "instanceSchema": ""}, "input_text_artifacts": {"metadataPath": "/tmp/inputs/input_text_artifacts/data",
          "schemaTitle": "system.Artifact", "instanceSchema": ""}}, "outputParameters":
          {}, "outputArtifacts": {"output_metrics": {"schemaTitle": "system.Dataset",
          "instanceSchema": "", "metadataPath": "/tmp/outputs/output_metrics/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.7
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-output-directory}
      artifacts:
      - {name: load-dataset-2-output_labels_artifacts, path: /tmp/inputs/input_labels_artifacts/data}
      - {name: train-output_model, path: /tmp/inputs/input_model/data}
      - {name: load-dataset-2-output_text_artifacts, path: /tmp/inputs/input_text_artifacts/data}
    outputs:
      artifacts:
      - {name: eval-model-output_metrics, path: /tmp/outputs/output_metrics/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.7.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.7.0
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: load-dataset
    container:
      args:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location
        'pandas' 'openpyxl' 'fsspec' 'kfp==1.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet                 --no-warn-script-location 'pandas'
        'openpyxl' 'fsspec' 'kfp==1.7.0' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        from kfp.v2.dsl import *
        from typing import *

        def load_dataset(url: str, num_samples: int, output_labels_artifacts: Output[Artifact],
                         output_text_artifacts: Output[Artifact]):
            import pandas as pd
            import pickle
            import numpy as np

            df = pd.read_csv(url, usecols = [6, 9])  # , nrows=num_samples)

            df.columns = ['rating', 'title']

            if num_samples > 1:
                df = df.sample(n = num_samples)

            text = df['title'].tolist()
            text = [str(t).encode('ascii', 'replace') for t in text]
            text = np.array(text, dtype = object)[:]

            labels = df['rating'].tolist()
            labels = [1 if i >= 4 else 0 if i == 3 else -1 for i in labels]
            labels = np.array(pd.get_dummies(labels), dtype = int)[:]

            with open(output_labels_artifacts.path, "wb") as file:
                pickle.dump(labels, file)

            with open(output_text_artifacts.path, "wb") as file:
                pickle.dump(text, file)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - load_dataset
      - --url-output-path
      - '{{$.inputs.parameters[''url'']}}'
      - --num-samples-output-path
      - '{{$.inputs.parameters[''num_samples'']}}'
      - --output-labels-artifacts-output-path
      - '{{$.outputs.artifacts[''output_labels_artifacts''].path}}'
      - --output-text-artifacts-output-path
      - '{{$.outputs.artifacts[''output_text_artifacts''].path}}'
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, load-dataset, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-output-directory}}',
        --enable_caching, $(ENABLE_CACHING), --, 'num_samples={{inputs.parameters.num_samples}}',
        'url={{inputs.parameters.url_train}}', --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.7'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"num_samples": {"type":
          "INT"}, "url": {"type": "STRING"}}, "inputArtifacts": {}, "outputParameters":
          {}, "outputArtifacts": {"output_labels_artifacts": {"schemaTitle": "system.Artifact",
          "instanceSchema": "", "metadataPath": "/tmp/outputs/output_labels_artifacts/data"},
          "output_text_artifacts": {"schemaTitle": "system.Artifact", "instanceSchema":
          "", "metadataPath": "/tmp/outputs/output_text_artifacts/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.7
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: num_samples}
      - {name: pipeline-name}
      - {name: pipeline-output-directory}
      - {name: url_train}
    outputs:
      artifacts:
      - {name: load-dataset-output_labels_artifacts, path: /tmp/outputs/output_labels_artifacts/data}
      - {name: load-dataset-output_text_artifacts, path: /tmp/outputs/output_text_artifacts/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"num_samples": "{{inputs.parameters.num_samples}}",
          "url": "{{inputs.parameters.url_train}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.7.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.7.0
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: load-dataset-2
    container:
      args:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location
        'pandas' 'openpyxl' 'fsspec' 'kfp==1.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet                 --no-warn-script-location 'pandas'
        'openpyxl' 'fsspec' 'kfp==1.7.0' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        from kfp.v2.dsl import *
        from typing import *

        def load_dataset(url: str, num_samples: int, output_labels_artifacts: Output[Artifact],
                         output_text_artifacts: Output[Artifact]):
            import pandas as pd
            import pickle
            import numpy as np

            df = pd.read_csv(url, usecols = [6, 9])  # , nrows=num_samples)

            df.columns = ['rating', 'title']

            if num_samples > 1:
                df = df.sample(n = num_samples)

            text = df['title'].tolist()
            text = [str(t).encode('ascii', 'replace') for t in text]
            text = np.array(text, dtype = object)[:]

            labels = df['rating'].tolist()
            labels = [1 if i >= 4 else 0 if i == 3 else -1 for i in labels]
            labels = np.array(pd.get_dummies(labels), dtype = int)[:]

            with open(output_labels_artifacts.path, "wb") as file:
                pickle.dump(labels, file)

            with open(output_text_artifacts.path, "wb") as file:
                pickle.dump(text, file)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - load_dataset
      - --url-output-path
      - '{{$.inputs.parameters[''url'']}}'
      - --num-samples-output-path
      - '{{$.inputs.parameters[''num_samples'']}}'
      - --output-labels-artifacts-output-path
      - '{{$.outputs.artifacts[''output_labels_artifacts''].path}}'
      - --output-text-artifacts-output-path
      - '{{$.outputs.artifacts[''output_text_artifacts''].path}}'
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, load-dataset-2, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-output-directory}}',
        --enable_caching, $(ENABLE_CACHING), --, 'num_samples={{inputs.parameters.num_samples}}',
        'url={{inputs.parameters.url_test}}', --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.7'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"num_samples": {"type":
          "INT"}, "url": {"type": "STRING"}}, "inputArtifacts": {}, "outputParameters":
          {}, "outputArtifacts": {"output_labels_artifacts": {"schemaTitle": "system.Artifact",
          "instanceSchema": "", "metadataPath": "/tmp/outputs/output_labels_artifacts/data"},
          "output_text_artifacts": {"schemaTitle": "system.Artifact", "instanceSchema":
          "", "metadataPath": "/tmp/outputs/output_text_artifacts/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.7
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: num_samples}
      - {name: pipeline-name}
      - {name: pipeline-output-directory}
      - {name: url_test}
    outputs:
      artifacts:
      - {name: load-dataset-2-output_labels_artifacts, path: /tmp/outputs/output_labels_artifacts/data}
      - {name: load-dataset-2-output_text_artifacts, path: /tmp/outputs/output_text_artifacts/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"num_samples": "{{inputs.parameters.num_samples}}",
          "url": "{{inputs.parameters.url_test}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.7.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.7.0
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: nnlm-model-download
    container:
      args:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location
        'tensorflow' 'tensorflow_hub' 'kfp==1.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet                 --no-warn-script-location 'tensorflow'
        'tensorflow_hub' 'kfp==1.7.0' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        from kfp.v2.dsl import *
        from typing import *

        def nnlm_model_download(untrained_model: Output[Artifact]):
            import tensorflow as tf
            import tensorflow_hub as hub

            hub_layer = hub.KerasLayer(
                "https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1", output_shape = [128], input_shape = [],
                dtype = tf.string, name = 'input', trainable = False
            )

            model = tf.keras.Sequential()
            model.add(hub_layer)
            model.add(tf.keras.layers.Dense(64, activation = 'relu'))
            model.add(tf.keras.layers.Dense(3, activation = 'softmax', name = 'output'))
            model.compile(
                loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = ['accuracy']
            )
            model.summary()
            print("\n\nsave untrained_model.pickle\n\n")
            model.save(untrained_model.path)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - nnlm_model_download
      - --untrained-model-output-path
      - '{{$.outputs.artifacts[''untrained_model''].path}}'
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, nnlm-model-download, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-output-directory}}',
        --enable_caching, $(ENABLE_CACHING), --, --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.7'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {}, "inputArtifacts":
          {}, "outputParameters": {}, "outputArtifacts": {"untrained_model": {"schemaTitle":
          "system.Artifact", "instanceSchema": "", "metadataPath": "/tmp/outputs/untrained_model/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.7
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-output-directory}
    outputs:
      artifacts:
      - {name: nnlm-model-download-untrained_model, path: /tmp/outputs/untrained_model/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.7.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.7.0
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: train
    container:
      args:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location
        'tensorflow' 'pandas' 'kfp==1.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet                 --no-warn-script-location 'tensorflow'
        'pandas' 'kfp==1.7.0' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        from kfp.v2.dsl import *
        from typing import *

        def train(epochs: int,
                  batch_size: int,
                  input_labels_artifacts: Input[Artifact],
                  input_text_artifacts: Input[Artifact],
                  input_untrained_model: Input[Artifact],
                  output_model: Output[Artifact],
                  output_history: Output[Artifact],
                  output_metrics: Output[Dataset]
                  ):
            import tensorflow as tf
            import pickle
            import pandas as pd

            print("Training the model ...")

            with open(input_labels_artifacts.path, "rb") as file:
                y_train = pickle.load(file)

            with open(input_text_artifacts.path, "rb") as file:
                x_train = pickle.load(file)

            # model = get_model()
            model = tf.keras.models.load_model(input_untrained_model.path)

            history = model.fit(
                x_train, y_train, batch_size = batch_size, epochs = epochs, verbose = 1, validation_split = 0.2,
            )
            loss, acc = model.evaluate(x = x_train, y = y_train)
            print("\n\n\n Loss = {}, Acc = {} ".format(loss, acc))
            metrics = pd.DataFrame(
                list(
                    zip(
                        ['train_loss', 'train_accuracy'], [loss, acc]
                    )
                ), columns = ['Name', 'val']
            )
            model.save(output_model.path)
            with open(output_history.path, "wb") as file:
                pickle.dump(history.history, file)
            metrics.to_csv(output_metrics.path, index = False)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - train
      - --epochs-output-path
      - '{{$.inputs.parameters[''epochs'']}}'
      - --batch-size-output-path
      - '{{$.inputs.parameters[''batch_size'']}}'
      - --input-labels-artifacts-output-path
      - '{{$.inputs.artifacts[''input_labels_artifacts''].path}}'
      - --input-text-artifacts-output-path
      - '{{$.inputs.artifacts[''input_text_artifacts''].path}}'
      - --input-untrained-model-output-path
      - '{{$.inputs.artifacts[''input_untrained_model''].path}}'
      - --output-model-output-path
      - '{{$.outputs.artifacts[''output_model''].path}}'
      - --output-history-output-path
      - '{{$.outputs.artifacts[''output_history''].path}}'
      - --output-metrics-output-path
      - '{{$.outputs.artifacts[''output_metrics''].path}}'
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, train, --pipeline_name, '{{inputs.parameters.pipeline-name}}',
        --run_id, $(KFP_RUN_ID), --run_resource, workflows.argoproj.io/$(WORKFLOW_ID),
        --namespace, $(KFP_NAMESPACE), --pod_name, $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID),
        --pipeline_root, '{{inputs.parameters.pipeline-output-directory}}', --enable_caching,
        $(ENABLE_CACHING), --, 'batch_size={{inputs.parameters.batch_size}}', 'epochs={{inputs.parameters.epochs}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.7'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"batch_size": {"type":
          "INT"}, "epochs": {"type": "INT"}}, "inputArtifacts": {"input_labels_artifacts":
          {"metadataPath": "/tmp/inputs/input_labels_artifacts/data", "schemaTitle":
          "system.Artifact", "instanceSchema": ""}, "input_text_artifacts": {"metadataPath":
          "/tmp/inputs/input_text_artifacts/data", "schemaTitle": "system.Artifact",
          "instanceSchema": ""}, "input_untrained_model": {"metadataPath": "/tmp/inputs/input_untrained_model/data",
          "schemaTitle": "system.Artifact", "instanceSchema": ""}}, "outputParameters":
          {}, "outputArtifacts": {"output_history": {"schemaTitle": "system.Artifact",
          "instanceSchema": "", "metadataPath": "/tmp/outputs/output_history/data"},
          "output_metrics": {"schemaTitle": "system.Dataset", "instanceSchema": "",
          "metadataPath": "/tmp/outputs/output_metrics/data"}, "output_model": {"schemaTitle":
          "system.Artifact", "instanceSchema": "", "metadataPath": "/tmp/outputs/output_model/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.7
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: batch_size}
      - {name: epochs}
      - {name: pipeline-name}
      - {name: pipeline-output-directory}
      artifacts:
      - {name: load-dataset-output_labels_artifacts, path: /tmp/inputs/input_labels_artifacts/data}
      - {name: load-dataset-output_text_artifacts, path: /tmp/inputs/input_text_artifacts/data}
      - {name: nnlm-model-download-untrained_model, path: /tmp/inputs/input_untrained_model/data}
    outputs:
      artifacts:
      - {name: train-output_history, path: /tmp/outputs/output_history/data}
      - {name: train-output_metrics, path: /tmp/outputs/output_metrics/data}
      - {name: train-output_model, path: /tmp/outputs/output_model/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"batch_size": "{{inputs.parameters.batch_size}}",
          "epochs": "{{inputs.parameters.epochs}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.7.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.7.0
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: train-amazon-pipeline
    inputs:
      parameters:
      - {name: batch_size}
      - {name: epochs}
      - {name: num_samples}
      - {name: pipeline-name}
      - {name: pipeline-output-directory}
      - {name: url_test}
      - {name: url_train}
    dag:
      tasks:
      - name: eval-model
        template: eval-model
        dependencies: [load-dataset-2, train]
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-output-directory, value: '{{inputs.parameters.pipeline-output-directory}}'}
          artifacts:
          - {name: load-dataset-2-output_labels_artifacts, from: '{{tasks.load-dataset-2.outputs.artifacts.load-dataset-2-output_labels_artifacts}}'}
          - {name: load-dataset-2-output_text_artifacts, from: '{{tasks.load-dataset-2.outputs.artifacts.load-dataset-2-output_text_artifacts}}'}
          - {name: train-output_model, from: '{{tasks.train.outputs.artifacts.train-output_model}}'}
      - name: load-dataset
        template: load-dataset
        arguments:
          parameters:
          - {name: num_samples, value: '{{inputs.parameters.num_samples}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-output-directory, value: '{{inputs.parameters.pipeline-output-directory}}'}
          - {name: url_train, value: '{{inputs.parameters.url_train}}'}
      - name: load-dataset-2
        template: load-dataset-2
        arguments:
          parameters:
          - {name: num_samples, value: '{{inputs.parameters.num_samples}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-output-directory, value: '{{inputs.parameters.pipeline-output-directory}}'}
          - {name: url_test, value: '{{inputs.parameters.url_test}}'}
      - name: nnlm-model-download
        template: nnlm-model-download
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-output-directory, value: '{{inputs.parameters.pipeline-output-directory}}'}
      - name: train
        template: train
        dependencies: [load-dataset, nnlm-model-download]
        arguments:
          parameters:
          - {name: batch_size, value: '{{inputs.parameters.batch_size}}'}
          - {name: epochs, value: '{{inputs.parameters.epochs}}'}
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-output-directory, value: '{{inputs.parameters.pipeline-output-directory}}'}
          artifacts:
          - {name: load-dataset-output_labels_artifacts, from: '{{tasks.load-dataset.outputs.artifacts.load-dataset-output_labels_artifacts}}'}
          - {name: load-dataset-output_text_artifacts, from: '{{tasks.load-dataset.outputs.artifacts.load-dataset-output_text_artifacts}}'}
          - {name: nnlm-model-download-untrained_model, from: '{{tasks.nnlm-model-download.outputs.artifacts.nnlm-model-download-untrained_model}}'}
  arguments:
    parameters:
    - {name: epochs, value: '10'}
    - {name: batch_size, value: '12'}
    - {name: num_samples, value: '10000'}
    - {name: url_train, value: 'https://www.dropbox.com/s/tdsek2g4jwfoy8q/train.csv?dl=1'}
    - {name: url_test, value: 'https://www.dropbox.com/s/tdsek2g4jwfoy8q/test.csv?dl=1'}
    - {name: pipeline-output-directory, value: ''}
    - {name: pipeline-name, value: pipeline/train_amazon_pipeline}
  serviceAccountName: pipeline-runner
